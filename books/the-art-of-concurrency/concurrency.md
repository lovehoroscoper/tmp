The Art of Concurrency
===

# Chapter 1

- 並列と並行の違い
	- 並行：システムが複数の動作を同時に実行状態(in progress)に保てる（i.e. 同時に実行しうる）
	- 並列：システムが複数の動作を同時に実行できる
	- 並列 ⊂ 並行
	- http://freak-da.hatenablog.com/entry/20100915/p1
- 並行プログラミングとは
	- システムが任意の順序で実行可能な独立した処理にまつわるすべて
	- ループや関数コールに依存しない独立した処理
	- 切り出せればなんでもスレッド（プロセスの内部）や連動プロセス（e.g. 子プロセス）に割り当てられる
	- 実装にはスレッドライブラリが必要で、それが行うスレッドの管理処理はどうしてもオーバーヘッドになる
	- スレッドはメモリを共有するので競合に配慮する必要がある（排他制御）
	- 並行化を頑張っても、少なからず逐次実行の部分は残る
- スレッド化の4ステップ
	1. 分析
		- アプリ内の独立可能な処理を特定する
		- アプリの理解やアルゴリズムの分析は当然重要
		- プロファイリングによってホットスポット（i.e. ある程度の量を持つ処理部分）を発見して、それが並行化可能か検討してくこともよいアプローチ
	2. 設計と実装
		- この本でやります
	3. 正当性の評価
		- スレッド化によって生じた（逐次実行時は存在しえなかった）エラーを発見する
		- データ競合やデッドロック、ポインタに拠る間接参照の並行化が引き起こすエラー
		- 計算結果のマージ時の順序によって最終的な値が異なることも：丸め誤差エラー
	4. 性能チューニング
		- 実行時間の短縮を頑張る
		- 効果的でないスレッドへの処理割当や各スレッドに課される異常なオーバーヘッドの存在
		- スレッド化そのものが逐次処理での最適化を壊してしまい、非効率なキャッシュ利用やメモリアクセスパターンによって性能低下を招くことも
- 共有メモリプログラミングと分散メモリプログラミング
	- 共通する機能
		- 全スレッドで実行される逐次処理というものは残ってしまうが、同一リソースを参照して各スレッドが値を保持することで不要な同期は避けられる
		- 実行のスレッド/プロセス分担と、データのスレッド/プロセス分担がある
		- データ共有の必要があることも；この場合分散メモリでは送受信が必要になる
		- 割り当てには静的/動的の二種類がある
			- 通常は静的割り当てのほうが実装も保守も単純なのでよい
			- 実行時間の事前見積りが不可能な場合は動的割り当てを考える；これは割り当てオーバーヘッドができてしまうがロードバランスの点で有利
	- 異なる機能
		- スレッド内のローカル変数；分散メモリではノード内にローカルメモリをもつ
		- 同一コア上の複数スレッドではメモリ共有による性能問題が発生しうる
		- 分散メモリではデータの送受信が必要
		- 共有メモリでは排他制御が必要
		- 共有メモリでは共有キューを用いたProducer-Comsumerアルゴリズムが有効
			- 自らBossプロセスにメッセージを送信して取りに行くBoss-Workerアルゴリズムは多数の同期が必要で辛い
			- 共有キューの排他制御は必要
		- 共有メモリではリードライトロックを使う
			- 複数スレッドが同一変数を参照するだけならいいけど、それを排他制御するとボトルネックになりうる。でも排他制御は必要
			- R/W Lock
				- 複数スレッドの共有変数の参照を許可
				- 更新する時はすべての参照スレッドのアクセスが完了していて、更新スレッドが自分だけであることを保証する
				- 参照頻度＞＞＞更新頻度、のときには利用すると良い
		
# Chapter 2

- タスク分解とデータ分解
	- 何をどう分解すればいいかは問題による
- タスク分解にはオーバーヘッドが付くので、可能なだけ完全に（全コア使って）並列化することが最適解とは限らない
	- コンテキストスイッチによって遅くなってむしろ悪くなることも
	- TLSの話
	- 非効率なメモリアクセス（謎） Locality
- どうやって分割しますか
	- 原則1：アイドルスレッドを発生させない
	- 原則2：粒度（？）
- 完全に並列化オッケーの場合もある：どっちも共有部分をReadしかしない
- 文化する処理がスレッドセーフかどうかに注意
- 動的割り当て：タスクの実行時間が読めなくて各処理がバラバラの処理時間のとき（サーバの処理とか；ロードバランス）
- 静的割り当て：実行時間が読める時はこっち
- Boss-worker　ランデブー　新しい処理を受け取るときの待ち合わせ（同期をとる）
- 例：数値積分でどこを並列化する？→依存性のないところをさがして分割する
	- 0 - num_rects の区間を適当に区切ってスレッドごとに部分和をTLS上に計算→待ち合わせて最終的な和をもとめる
- データ分割どうする？
	- 2x2 の配列があったときに列で分割する？行で分割する？…分割方法いろいろ
	- 経験的に一番よかったのは「分割されたブロック同士が接する」：そういう問題がおおい e.g. ライフゲーム、熱伝搬
		- ダミー状態を保持する必要がでてくる
		- データ構造で解決する/同期をとる
- 並列化するときに考えるべきはスケーラビリティ＞実行効率
	- スケーラビリティが考慮されていれば、ハードウェア的に良くすれば性能も比例して上がるから
- 並列化不可能な問題もどうしてもあるということは知っておくべき
	- 状態を持つアルゴリズムはむり（ステートフル；漸化式的なアルゴリズム）
	- 帰納変数が存在すると並列化不能
	- リダクション（配列の要素を1つの実数にまとめる）
		- 順不同ではないループはリダクション不可
	- ループ内依存は消せないこともないけど無理やりやるのも苦しいし、大抵の場合は無理です
	
# Chapter 3

- 並列化手法の最後の2ステップを説明する
	- 分析 (Ch. 2) => 設計・実装（このあと） => 正当性の評価 => 性能チューニング
	- 正当性の評価：並行アルゴリズムが正しく動作してる？誤り無く設計できた？
	- 性能チューニング：コードが十分並列に動作しているか確認
- Ben-Ariさんが一般化した並行アルゴリズムの備えるべき正当性・性質4つに沿って評価する
	1. プログラムとは連続したアトミックな実行文である
		- 各実行文がそれ以上小さな命令に分解できない
		- 実行完了まで割り込まれない実行文
	2. 並行プログラムは複数のスレッド内のアトミックな実行文のインタリーブ（相互の挟み込み）である
	3. アトミックな実行文のすべての組み合わせは、ここで検証する並行アルゴリズムのすべての性質を満たさなければならない
		- 複数スレッド􏲉内のアトミックな実行文のすべてのインタリーブ􏳈が期待される状態を維持していることを証明する
		- 例：2スレッドでそれぞれ2つのアトミックな実行文があるとき
			- s1 -> s2 の順序は保持するインターリブを考えると6通りできる
			- 期待する状態はどれ？きちんとその状態になることが証明できる？
	4. どのインタリーブでもスレッドの実行文が(不公平に)除外されることはない
- アトミックな文のインタリーブの実際を把握して、その検証プロセスの一端を感じて欲しい
- クリティカルセクション（クリティカルリージョン）問題
	- クリティカルセクション：共有変数を参照・変更するソースコード部分
	- 備えるべき（検証したい）性質
		1. 排他制御は必須。共有変数は参照/変更問わず同時に1スレッドからしかアクセスされない
		2. スレッドがクリティカルリージョン外を実行中なら、そのスレッドは他のスレッドがクリティカルリージョンへ入ることを妨げてはいけない
	- 段階的に、Dekkerのアルゴリズムに基づく解法を例示する
	- 簡略化のため2スレッド（T0とT1）がクリティカルリージョンに入ることを考える
	- **第1段階: 共有変数1つを使ったナイーブな排他制御実装**
		- 排他制御は保証されるが *交互実行の強制* という問題がある
		- 例えばこれをシングルコア上で実装した場合、
			1. T0: threadNumber=1
			2. T0: other
			3. T0: while()...
		- のような T0 が threadNumber 更新後に 更に先に処理を行うインタリーブが発生すればT0, T1両方が待つことになる
		- インタリーブ依存でライブロックが起こりうる
		- 片方が待ってるときに、他方のスレッドが死んだらそれもデッドロック
	- **第2段階: クリティカル中にいるかを示すフラグを置く**
		- 第1段階での問題は解決するが、今度は *排他制御が保証されない*
		- T0_isin = T1_isin = 0 のときに、 
			1. T0_checkT1 (pass)
			2. T1_checkT0 (pass)
			3. T0_isin=1
			4. T1_isin=1
		- というインタリーブになると、両者同時にクリティカルを実行できる
		- "自分勝手"なスレッド（自分のやってることだけ明示すればいいと思ってる）
	- **第3段階: フラグを check の前に更新する**
		- もう少し礼儀正しいスレッド（入りたい意思を持った時点でそれを相手に伝える）
		- たとえば、
			1. T0_wantEnter=1
			2. T1_wantEnter=1
			3. T0_checkT1...
		- のようなインタリーブで *デッドロック* が起こりうる
		- デッドロック発生の必要条件4つのうちいずれかを潰せば解消される（全部満たされている＝デッドロック）
			1. 相互排除 (mutual exec.)
				- 各資源は1つのプロセスのみに割当てられているか、または利用可能である
				- どっちも待ってるので、「実行したい」かつ「割り当てられていない」資源が利用可能ではない
			2. 獲得後のウェイト
				- 現時点でリソースの所有を認められているプロセスが更に新しいリソースの要求が出来る
			3. プリエンプトなし
				- 資源の横取り不可
			4. 循環待ち
				- 待機資源が循環している．
				- {p0, p1, p2, …., pn} で、p0がp1の確保している資源を待つ
	- **第4段階: タイミングをずらしてデッドロックを解消する**
		- T0でほしいって言ったのに、T1でもほしいっていう状態（獲得後のウェイト）を潰す
		- T0とT1で異なるインターバルを設定して、waiting中に wantEnter の一時的な off 状態を作る
		- 以下のとき、T0はT1を締め出してしまう（T1のインターバルが長すぎて、T0は何も実行していないはずなのに、T1のcritical実行を妨げてしまう）
			1. T1_startDelay
			2. T0_critical
			3. T0_wantEnter=0
			4. T0_other
			5. T0_wantEnter=1
			6. T1_endDelay「まだT1はwantだ」
		- これをスタベーションという
	- **Dekkerのアルゴリズム: 礼儀正しさやランダム性に依存せず、優先権でスレッドの排他制御をコントロールする**
		- デッドロックが発生したときに優先的に実行すべきスレッドを favored フラグ決めておく
		- favoredフラグは、Critical実行終了後に他方に譲る
		- インタリーブの組み合わせはたくさんあるが、大きくわけると4種類。それぞれについて正当性を示せばいい
		1. 競合なし
			- 先の例の通り動く
		2. デッドロック発生、T0に優先権あり
			- T1は最初待つ。T0_critical 後に必ず優先権はT1に移るので、その後 T0の処理が続いたとしても、T1_endDelayのときには T0が待つ必要ある
		3. デッドロック発生、T1に優先権あり
			- 同上
		4. 共に want なとき
			- T0_want -> T1_want で両方whileに入っても、優先権ある方が先に抜ける
	- 2スレッド問題にしか対応できないのは問題です 
	- 優れたアルゴリズムは他にもあるけど、アトミックな文のインタリーブを検証することの気持ちを感じてくれることが主眼
	
- 性能評価
- 高速化率：逐次実行と並列実行の実行時間の比率
	- 逐次実行は十分に最適化されていること
	- いろいろなコア数で試すことはスケーラビリティを図る上で重要
	- コア数が倍なら、実行速度も倍になるべき（＝スケーラブルな状態）
	- コア数2倍で速度3倍、とかになったら誤りを疑う（スーパーリニア高速化率）
		- たとえば入力データが少なすぎてローカルキャッシュに収まってしまった時とか
	- 高速化率には上限がある(Amdahlの法則)
		-	逐次処理内容のうち並列実行可能な部分の割合: R（並列実行される関数などがわかっていれば、プロファイル結果からRを割り出せる）
		- 高速化率は 1 / ((1 - R) + (R / コア数)) 以下
		- R = 0.75 で8コアでも、高速化率の上限は3倍にしかならん
		- 処理の100％が並列化可能だと、ようやくコア数に比例して高速化していくようになる（完全にスケーラブル）
		- 教訓
			1. 最終的には逐次実行に依存すること
			2. それでも可能な限り並列化できたほうがよいということ
		- ただしオーバーヘッドを考慮していない
		- 実際にはコア数増加に伴って処理可能なデータ量も増えていくはず
	- Gustafson-Barsisの法則
		- 既存の並列コードの高速化率
	- Amdahl: 逐次があって、並列化したらどれくらい速くなる？
	- Gustafson: 並列があって、逐次のときよりどれだけ早いといえる？
- 実行効率：システムの計算リソースをどれだけ上手に消費しているか
	- 高速化率 / 使用したコア数
		- 64コアで53倍の高速化率→実行効率は 53 / 64 = 0.828 で82.8%
		- 処理全体を平均すると実行時間のうち約17%は全コアがアイドル状態
		- コアを使い切れなかった理由（待ち？同期？競合？）は様々なので、「コア数減らせば実行効率100％！」とは言えない
		- 原因を定式化するには組み合わせが多すぎるので、実際には複数パターンで計測してアプリケーション、スレッドの動作を考察する必要アリ
- HT: プロセッサ内のレジスタやパイプライン回路の空き時間を有効利用して、1つのプロセッサをあたかも2つのプロセッサであるかのように見せかける技術。

# Chapter 4

- 並列化手法の最後の2ステップを説明する
	- 分析 (Ch. 2) => 設計・実装（ここ） => 正当性の評価 (Ch. 3) => 性能チューニング (Ch. 3)
	- 設計の8つのルールを紹介
	
1. 真に独立した処理を特定する
	- 処理内容が互いに独立して実行可能でなければ並行実行は不可
	- Ch. 2で述べたように、並列化不可能な部分は少なからず存在する
2. 並行性はより上位で実装する
	- ホットスポットが低いレベルにあると、上位でそれを包括する更に思いホットスポットが現れる
		- e.g. セルがホットスポット→フレームはたくさんのセルを含む→動画は複数のフレームからなる
	- ホットスポットを含む範囲をできる限り上位で並行化する
3. コア数増加に備えて、スケーラビリティ対応を早期に計画する
	- コア数が増えて処理能力が上がれば、処理できるデータ量は増える
	- 処理するデータはどこかで誰かが必ず持っている
	- データ分解による並行性の設計・実装はスケーラビリティ的にやりやすい
4. スレッドセーフなライブラリを使用する
	- ライブラリ内の共有変数へのアクセスがデータ競合を引き起こす場合はある
	- 並行実行部分でのすべてのライブラリ関数がスレッドセーフであることを確認
	- 自作ライブラリを使用する場合はリエントラント（複数スレッドから同時に呼び出されても正しく動作する）関数をつくる
	- 無理なら共有変数を保護する同期処理を書く
5. 適切なスレッドモデルを採用する
	- e.g. OpenMP はデータ分解に特価
	- over implement なモデルは誤りの発見が困難になるだけで辛い
6. 実行順序を前提としない
	- 並列化するとどうしても nondeterministic な処理順序になる
	- 結果としてデータ競合やブロックなどによる遅延の有無、それに伴う性能低下が招かれる
	- 実行順序に制約を加えたいのなら同期が必要だし、このことを肝に銘じておくこと
7. できればロックで保護されたローカル変数を使用する
	- 同期処理はオーバーヘッドを伴う必要悪
	- スレッドのローカルメモリやスレッドIDでインデックスした配列などを代用して、同期処理を最小限に留める
	- どうしてもロックするなら1つのデータに対して複数のロックを対応づけるのはご法度
8. 並行性向上のためのアルゴリズム変更を恐れない
	- 計算量は落ちても、並行化できるのならそのほうが良い場合もある
	
# Chapter 5

- pthread
- pythonではどうなってる？

# Chapter 6

- リダクションの並行アルゴリズムはデータ分解が基本

# Chapter 7

- "MapReduce" というフレームワーク
- Map
	- key-valueペアの生成
	- ペア数は元のデータ数と等しくなる
	- データ毎に完全に独立した処理なので並行性はOK
- Reduce
	- Mapの結果得られたペアたちに対するリダクション
- 例7-1：文字列の各アルファベットに対して
	- Map: 子音なら<char,1>、母音なら<char,-1>のペア生成
	- Reduce: valueが正なら子音カウンタに加算、負なら母音カウンタに絶対値を加算
	- データがアルゴリズムから分離される（i.e., data-dependent rather than algorithm-dependent, 問題を解くアルゴリズム依存ではなく、データ単位で処理を行う実装になっている）のでリダクションの独立性が高まる
- Map処理は文字（データ）ごとに独立なので、複数スレッドに文字列を分割して渡しても動く
- このときReduceを可能な限り単純化することが重要
	- コンテキスト（意味のある情報）は分割する側（Map側）で与える
	- すべての文字をMapで<char,1>のペアにして、Reduceでcharの母音/子音を判断→適切なカウンタを加算、ということもできるが、これだとMapでのデータ分割単位 char（キー）に係るコンテキストが使えない
	- 例："y"という文字はコンテキスト（単語）に応じて母音/子音の両方になりうる。Mapの時点でコンテキストに応じて+1,-1を判断してあげればそれで済むけど、Reduceまで判定を持ち越すと、場合によってはReduceに来た情報だけでは母音/子音の判断がつかなくて元の単語が全て必要、ということになりかねなくて無駄
- 並行Map処理の実装（アプリケーションごとの差が大きいので一般的な話のみ）
	- Map＝データ分解
	- Reduceを念頭に設計する
		- Mapの実装が厚い→Reduceが軽くなってMapReduce全体の開発・保守が楽になる
		- 最適なReduce＝個々のデータにも、部分リダクション結果にも適用可能な単一の処理
			- 例：値ではなく正負だけをみてカウンタを更新していた母音/子音判定
		- 個々のデータのMapは他のデータとは独立→競合は発生しないはず
			- もし同期が必要になったら、Map処理の再検討、Reduceでの競合対応、そもそも問題がMapReduceに適しているか、を考える
		- Mapのロードバランスにも注意
			- 個々のデータのMap処理の時間が均一でなければ、スレッドの動的スケジューリングをしたい（e.g., データが長さの異なる文書）
- 並行Reduce処理
	- 部分和をまとめていく、すなわちリダクション（前章）なので、スレッドライブラリ使っていこう
	- PRAM（1データ1スレッドのモデル）は前の段での部分和がすべて求まっていないと次を計算できないのに一切同期をとっていなくて、現実的には残念なモデルだった（壊れるインタリーブがすぐ見つかる）
		- 各スレッドでチャンクから部分和を求めて、それを集計する、という現実的なアルゴリズムで実装したのが例6-4
	- ここではバリアオブジェクトを設けて、部分和を求めている全スレッドがバリアに到達することを確認してから次に行く（gSumを更新する）、という実装で同期をとるように改善。PRAMモデルを実装するぞ
		- 待つスレッド (pthread_join) はid=0のスレッド、つまり最終的な和が入っているやつだけ
		- ループ
			0. バリアオブジェクトを置いて、他の全スレッドが部分和を求め終わるまで一時停止
			1. 自分のスレッドIDが2の倍数なら、部分和が入るスレッドなので1個先のスレッドの gSum を自分に加算
			2. バリアオブジェクトを置いて、他のスレッドが1個先のスレッドの gSum を加算し終わるまで待つ *（無関係なスレッドは if 文全部スルーしてバリアオブジェクトに到達してる）*
			3. 自分のスレッドIDが4の倍数なら、部分和が入るスレッドなので2個先のスレッドの gSum を自分に加算
			4. バリアオブジェクトを置いて、他のスレッドが2個先のスレッドの gSum を加算し終わるまで待つ
			5. ...
			6. スレッドid=0は最後まで ((tNum % p2) == 0) に引っかかるので、id=0の終了を待っていれば最終的な結果がわかる
	- スレッド数が奇数(e.g., 9)でも、溢れた部分はそのまま次の部分和計算に持ち越すだけなので動く
	- スレッド分割を (float) でキャストしていることに注目。intで丸めると、剰余が1つのスレッドに集中してロードバランス的に良くない。
- バリアオブジェクトの実装（初期）
	- *最後のスレッドがバリアを抜け出るまでバリアに到達したスレッド数を0のままにしてしまう実装*
	- バリアに到達した別スレッドが条件式でカウンタを見て、まだ0なので wait をスルーしてしまう **ウェイトの横取り**
	- ↓こんな感じ？

```c
mutex_lock(m);
b->count--;
while (b->count > 0) {
	thread_wait(c, m); // unlock mutex => wait
	// wakeup => lock mutex
	mutex_unlock(m);
} 
if (b->count == b->numThreads) {
	/**** 他のスレッドの待ちを解除してから ****/
	thread_broadcast(c); // wakeup
	b->count = b->numThreads;
	mutex_unlock(m); // ready to exit
	/**** バリアを出るまで。この間は b->count が 0 で、解除されたスレッドのいずれかが while をスキップし得る ****/
	
}
```
	
- バリアオブジェクト（改善版）
	- 色付けをしよう
	- ロック取得直後の状態を保持しておくことで、確実に状態が変わる（＝最後のスレッドがバリアを抜ける）まで再始動しないようにする
	- ↓待ち解除前に初期化できればいいのだから、つまりこれでもいいのでは？？？
	
```c
mutex_lock(m);
b->count--;
while (b->count > 0) {
	thread_wait(c, m); // unlock mutex => wait
	// wakeup => lock mutex
	mutex_unlock(m);
} 
if (b->count == b->numThreads) {
	b->count = b->numThreads;
	thread_broadcast(c); // wakeup
	mutex_unlock(m); // ready to exit
}
```

- 実行効率
	- リダクションではバリアの実装が大きく影響する
	- 最後のスレッドか否かを判断する分岐が必要なので、これはオーバヘッドになる
- スケーラビリティ
	- 自作バリアオブジェクトはスレッド数が多いとボトルネックになりうる
	
- MapReduceの応用
	- N以下の自然数から、すべてのfriendlyな組を見つける
		- a, b があったとき、 **(aの約数の和 / a) = (bの約数の和 / b)** のとき、a, b はfriendly
	- 逐次アルゴリズム
		- すべての自然数について約数とその和を求める
		- 約数の和 / その数 の **約分結果（小数だと数値誤差で正しくない可能性がある）** をすべて求める
		- 等しい組を見つける
	- MapReduceできる？
		- 独立した2つの処理に分解できるか
		- 最初の処理はデータ分解になるか
		- データが相互に独立しているか
		- データとキーの "マッピング" が考えられるか
	- Map
		- 各自然数 n というデータについて、(nの約数の和 / n) をマッピング
		- 因数分解、(nの約数の和 / n)の計算は他の自然数に非依存
	- Reduce
		- ペアを相互比較して一致するものを見つける
		- 1つの数字にまとめるわけではないが、大きなものから小さな値を得るという点で「リダクション」と捉えられる
	- OpenMPによる実装
		- Mapの処理量は一定ではないので dynamic スケジューリング
			- 約数は自分より下の数をすべて見ているので、大きい数ほど遅い
		- Reduceは内側ループの回数は一定の割合で減少して予測可能なので静的スケジューリング
			- 先頭のチャンクの処理量が一番多い
			- （ラウンドロビンでチャンクをスレッドに割り当てるので）チャンク数が多いほど、1スレッドあたりの処理量は均一になる
- MapReduceは汎用エンジンなので、簡単な問題ならエンジニアはリダクション用の比較関数のみを実装すればいい
	- 今後もっと強い汎用エンジンがでてくるかもね
	
# Chapter 8

## クイックソート

- ピボットを決めて、左にピボットより下の要素、右にピボット以上の要素を寄せる→ピボット前後で配列を左右に分けて再帰
- 案1：QuickSort() を再帰呼び出しする度にスレッド立ち上げる→最悪（末端までいくと、1要素1スレッドで地獄）
- とりあえずキュー（スタックでも一緒）を使って再帰を潰そう
	- 一度QuickSortでピボットの前後に分けたら、前後それぞれをキューにいれて、順番にQuickSortにかけていく
	- キューが空になったら終わり
- データ分解を考える
	- QuickSort() 間に依存は無い（1~mid と mid+1~end のソートなので）
	- → スレッドプールを用意してグワッと並列化できる
- スレッドプールを実現するために、以下の3つの方法を何とかして実現する必要がある
	1. すべての処理が完了したことをスレッドに通知
		- 並行なので「キューが空」で終了判定ができない
		- 最終的なソート位置に移動された要素（確定したピボット）数を追跡することで判断する
		- カウンタはアトミックに加算されてね
	2. スレッドへの処理の分配方法
		- キューを共有しておけば、「手が空いたらキューからとる」で分配していける
		- 共有キューの enqueue, dequeue はスレッドセーフに
		- あと空のときに enqueue しないような、安全な分配が必要
			- ただしループでスピンウェイトするとダメなインタリーブが出る
			- → キュー内のデータ数を表すセマフォオブジェクトを使おう
			- スレッドはキューが空になると自動でブロックする
	3. 処理完了時のプール内のスレッドの終了方法
		- ソート結果を待っている外部スレッドにスレッドプールのソート完了を通知しよう
		- 外部スレッドがプール内のスレッドを全部殺すことも出来るけど、共有データの更新中・同期オブジェクト保持中かもしれないのでよくない
		- 完了通知を受け取ったときに、スレッドがアルゴリズム的にどこの状態にあるのかはシビア
		- というわけでここでもセマフォを利用
			1. 終了通知が外部スレッドに飛ぶ
			2. Done フラグを true にする
			3. 終了したプールスレッドは queue 内の要素数を持つセマフォでブロック中
			4. 外部スレッド側でセマフォに適当な値（スレッド数）を入れる
			5. 終了したプールスレッドはブロックを抜けて、直後の Done フラグのチェックで処理を終了する
- 実行効率
	- 共有キューへのアクセスはオーバーヘッド
	- また、ピボットが配列のどの位置で確定するかは謎なので、ピボットの前後のデータ数が不均衡になることもある

## 基数ソート

- 桁ごとに並び替えて徐々に全体をソートしてく
- 今回は2進数なので桁＝ビット
- 基数交換ソート
	- 上位から下位へビットを見ていく
	- 最上位ビットが1のひとは、最上位0のひとより絶対大きい
	- 指定した数の上位bitだけを見て、
		1. 0 | 1
		2. 00 || 01 | 10 || 11
	- ってやる。step1 で0(1)だった人たちが更に 00/01 (10/11) に二分されるるので、（ピボットも次のソート対象に含む版の）クイックソートのパーティションの動きと全く同じ
	- スレッドのソート完了を示す状況は
		1. パーティションに要素が1つだけ
		2. 調べる次のビットが最下位ビットを超えた（全ビット見終えた）
- 直接基数ソート
	- 下位から上位へ
	- 安定（ソート後の相対順序が変化しない）
	- mbits ごとのプレフィックススキャンで実装できる
		1. 下位[1,mbits]をみてそのパターンの頻度をカウント: count[]
		2. count[] をプレフィックススキャン
		3. 最後の要素の下位mbitsを見ていって、tmp_arrayの該当するcount[]値のインデックスにコピー
		4. 該当するcount[]値をデクリメント
		5. 最後-1の要素の下位mbitsをみて…（繰り返し）
		6. 下位[mbits+1,mbits*2]に対して同じことをやる
	- 外側のループに依存してる＝下位mbitsのソートが終わっていないと、mbits+1~のソートができない
	- カウンタが配列でプレフィックススキャンしなきゃいけないのも並列化を妨げてる（順序を壊せない）ので以下でマシにする
		1. オフセット=0
		2. あるビットパターンについてカウント
		3. 末尾要素から順番に見ていって該当する要素をコピー
		4. デクリメント
		5. オフセット=今回のパターンのカウント
	- でも、「該当する要素をコピー」で
	
# Chapter 9: さーち

## バイナリサーチ

- 再帰でも書けるけど並行化しにくいのでやりません
- リニアサーチと同じ方法で書ける
	- データ分解
	- ヘルパーメソッドを立てて、スレッドはそれを呼ぶ
	- ヘルパーメソッドを内で各データチャンクについてサーチ＆グローバルな done フラグを見ながら return 判定
	- done は判定を逃しても1要素後にはもう一度判定して抜けられるので保護はいらない
	- 各スレッドの pos をチェックして結果を調べる
	- 計算量、リニアの O(n) に対して O(logn) でずっと早いので、見つけたスレッドが pos, doneフラグ変更後にbreak、とかもなくてもいいかもね
- あたりが1つだけだと、それを含まないチャンクをとったスレッドは皆全部見るだけみておわる。それでもよければこれでおわり。以下advanced
- N分探索：図9-1
	- （等間隔の）N要素について、探してる人と比較
	- 異なる方向を指す隣接要素(RR[RL]LL)をみつければ「どの要素とどの要素の間にいるか」が絞れる。そこを次のサーチ範囲として、またその中のN要素みる
	- イコールになる比較をどこかのスレッドが行えば終了
	- 次のサーチ範囲が空なら要素は存在しないとわかる
- 各スレッドがみた要素がキー以上か以下か、それを知るためのグローバルな要素は必要
- 各スレッドが要素の比較を完了したことバリアが必要
- 逐次実装
	- ステップ数はfloatにすることで均等に分割
	- 第1ループで各要素を探してる人と比較→R/L/Eのいずれかをセット
	- 第2ループで R-L となってる隣接要素を見つけて、そこを次の探索範囲にセット（R-EやE-Lでも引っかかって無駄に更新しちゃうけど）
	- 探索範囲 [lo, hi] が前後関係保ってる or pos==-1 （まだ未発見）の間繰り返す
- 並行化では、第1ループの後(locate)と、第2ループの後(lo,hi,pos)の2箇所でバリア同期をとること
- OpenMPのワークシェアリング構文でよしなに配列を等間隔に分割して分担実行してくれる
- 1スレッド1間隔が極端すぎれば、もうすこしたくさんの要素の比較を1スレッドにやってもらうこともできる。Ntrvlの値をスレッド数より多くすればいい
- 実行効率
	- Ntrvlいい感じに設定すれば、同じlocate, midを見てるスレッドがいなくなって良い(?)
	- とはいえ、同じところを見てしまうとmidの参照で偽共有が発生しうるので、lmid = mid[i] でスレッドローカルな変数にしてから参照するのがよい
	- 同期オブジェクトは使っていないので、効率としては粒度のみが問題。配列サイズ、スレッド数、コア数に依存します
- 可搬性
	- スレッドの処理は与えられた要素（1個〜）の比較のみなので、分散メモリプラットフォームなどを考えると、その都度発生する通信量のほうがはるかに重い
	- それなら各プロセスでローカルにバイナリサーチ走らせる、とかのほうがマシ
	
# Chapter 10: グラフアルゴリズム

- 深さ優先で全探索
- 実装で visit を記録するが、visit時の処理は目的による	
- 深掘りは再帰の実装が楽だけど、並行化を考えてクイックソート同様にfor実装にしましょう（スタック）
- クイックソート同様に、スレッドプールを使って「まだ訪れていないノードを訪れる」（ソートでは「パーティション」に相当）というタスクに分解して並行化可能
	- スレッドセーフなスタック
	- スタックの要素数をカウント・管理するためのセマフォ
	- 探索修了の判断・通知
	- 共有配列 visited へのアクセスを保護 (!!)
- クイックソートと異なり、visited の共有に対して設定すべきロックが必要
	- １つのスレッドがノードを訪れる度に配列全体を完全に（読み書き両方）ロックしてしまうと、待たされるスレッドが多すぎる説
	- 更新は1ノード（＝配列の1要素）あたり1回、参照はみんなからたくさん、という状況なら、Read/Write lockを使うとベター
		- クリティカルリージョン範囲が狭すぎる（i.e. コード上でクリティカルリージョンを参照する場所が小さすぎる）ので、ロック状態のアトミックな参照/更新のオーバーヘッドが重くて性能的にあまり良くない
		- 最大効率を得たければ、ロックのオーバーヘッドが埋もれるくらいの大量の参照をコード内の広い範囲で同時に行なって欲しい（例10-2では2行しかvisited使ってない）
	- というわけで、配列全体を対象とした1つのロックはあまりうれしくないので、要素1つ1つにそれぞれロックをかける方針を考えよう
		- 平均すれば、複数スレッドが同一ロックを見に行くことは、1ロック1配列の場合よりもずっと少ない
		- ただし、V要素に対してV個のロックオブジェクトが必要。メモリ容量（要素数に比例するロックオブジェクト）と性能（ロックのオーバーヘッドを潰す）のトレードオフ
		- モジュロロックを使って中間のロックオブジェクト数を設定する
- モジュロロック
	- データのインデックスをロック数で割った余りをロックインデックスとして、そのデータ用のロックオブジェクトを特定
	- ロックオブジェクト2個なら、偶数インデックスの要素はロックオブジェクト1、奇数インデックスの要素はロックオブジェクト2、と分けられて、配列全体を1つのロックで見るよりは倍マシ
	- では最適なロックオブジェクト数は？→経験上スレッド数に一致させればok
	- （最良のケースでは）すべてのスレッドが異なるロックオブジェクトを使用できる
	- まぁ一概には言えないけどね
- 一般に、競合時の待ち時間を少しでも短縮するために、クリティカルリージョンはコード内でコンパクトにまとめるべき
	- でも複数ロックオブジェクトを使うアドバンテージが大きすぎる（競合が劇的に減る！）ので、今回の設定ではクリティカルリージョンが広くてもさほど問題にならない
- というわけでvisitedの参照、更新をそれぞれ確実にmutex_lock
	- if文での参照もしっかりロック（同時に2つ以上visitしないように）
	- ローカル変数に現在のvisit statusをコピーする部分をロック
		- コピーしてから、visited更新のロックまでの間で、他のスレッドがコピーして、2つ以上のスレッドがvisited更新可能になるインターリーブが作れてしまう
		- 参照結果を最後まで確実に保証するために、1つのmutex_lockでifから更新までの範囲をロックしよう（例10−4；一度visited絡みの処理をまとめてロック内で行って、別のフラグで後の処理を決定）
- というわけで深さ優先の並行実装
	- スタックの実装やセマフォはクイックソート参照
	- あとvisit(訪れた)時のグローバルなカウンタが必要
		- モジュロロックだと、複数スレッドが1つのカウンタに同時にアクセスすることは許される場合もあるので、visited配列と同じレベルでカウンタをロックするのは良いとは言い切れない
		- まぁ windowsスレッド のinter lock使えば特に意識しなくてよい
- 探索終了＝スタックが空、とは言えない
	- 今の実装では、自分が未訪問なら、隣接要素をとりあえず全部スタックに入れる
	- 最後の未訪問ノードの処理終了後でも、隣接要素はスタックに一度入れられる
	- スタックが空であろうがなかろうがどうせ最後にはgCountで抜けてしまうし、セマフォいらんのでは？
	- しかし最後のノードが隣接要素を1つも持っていなければ探索終了＝空なので、やっぱりセマフォでおとなしく待っていて欲しい
- setEvent は do_something が確実に終わったあとでね（完全な探索結果を返すこと）
- 実行効率
	- モジュロロック良い
	- スタックの最大サイズはV^2（全部が全部と連結）なので、スタック実装は静的なリストで良いと思う。ノード1つの持つデータがあまりに大きいなら動的リストを検討
- スケーラビリティ
	- ノードを訪れる処理の計算量(Do something)がキモ
	- ここが重いほどスレッドの粒度を荒くできる（スレッド数増やせる？）
- 幅優先
	- ルービックキューブは幅優先のほうが早い
	- スタックがキューになる（スレッドセーフ）
- 世には動的グラフ（探索中にも、次の隣接ノードがその場で生成されて増える）も存在する e.g. ゲーム木
	- どこで終了とするか、は検討しなければならない